{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595911597388",
   "display_name": "Python 3.8.3 64-bit ('torchenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCorrect\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import vocab and Probability Dictionary\n",
    "First we will import the vocabulary of our autocorrect along with the probability dictionary of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./vocab_prob_dicts.p\", \"rb\") as f:\n",
    "    prob_dict, vocab_word_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit methods\n",
    "Now, that we have probabilities for all the words in the corpus, It's time to writa a few functions to manipulate strings so that we can edit the erroneous strings and return the right spellings of the words.\n",
    "We will implement four functions:\n",
    "\n",
    "- **delete_letter**: given a word, it returns all the possible strings that have one character removed.\n",
    "- **switch_letter**: given a word, it returns all the possible strings that have two adjacent letters switched.\n",
    "- **replace_letter**: given a word, it returns all the possible strings that have one character replaced by another different letter.\n",
    "- **insert_letter**: given a word, it returns all the possible strings that have an additional character inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word, verbose = False):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Input word : {word}\\nSplit list : {splits}\\nDelete list : {deletes}\")\n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input word : cans\nSplit list : [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')]\nDelete list : ['ans', 'cns', 'cas', 'can']\nNumber of outputs of delete_letter('at') is 2\n"
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\", verbose=True)\n",
    "\n",
    "print(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    switches = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) >= 2]\n",
    "    \n",
    "    if verbose:\n",
    "        if verbose: print(f\"Input word = {word} \\nsplit_l = {splits} \\nswitch_l = {switches}\") \n",
    "\n",
    "    return switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input word = eta \nsplit_l = [('', 'eta'), ('e', 'ta'), ('et', 'a')] \nswitch_l = ['tea', 'eat']\n"
    }
   ],
   "source": [
    "switch_word_l = switch_letter(\"eta\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose = False):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    string = \"abcdefghijklmnopqrstuvwxyz'\"\n",
    "    replaced_l = []\n",
    "    \n",
    "    for L, R in splits:\n",
    "        if R:\n",
    "            for letter in string:\n",
    "                replaced_l.append(L + letter + R[1:])\n",
    "            \n",
    "    replaced = set(replaced_l)\n",
    "    replaced.discard(word)\n",
    "    \n",
    "    replaced_l = sorted(list(replaced))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Input word = {word} \\nsplit_l = {splits} \\nreplace_l {replaced_l}\")\n",
    "    \n",
    "    return replaced_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input word = can \nsplit_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \nreplace_l [\"'an\", 'aan', 'ban', \"c'n\", \"ca'\", 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose = False):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    string = \"abcdefghijklmnopqrstuvwxyz'\"\n",
    "    \n",
    "    inserted_l = []\n",
    "    \n",
    "    for L, R in splits:\n",
    "        for letter in string:\n",
    "            inserted_l.append(L + letter + R)\n",
    "                \n",
    "    inserted = sorted(inserted_l)\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {splits} \\nreplace_l {inserted_l}\")\n",
    "        \n",
    "    return inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input word = at \nsplit_l = [('', 'at'), ('a', 't'), ('at', '')] \nreplace_l ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', \"'at\", 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', \"a't\", 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', \"at'\"]\nNumber of strings output by insert_letter('at') is 81\n"
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Edits\n",
    "Now that we have our edit functions, we can put them all together to get a list of all the possible strings from the input string by applying the functions defines above. We will get the strings which are `ONE` or `TWO` edits away from being a correcct string.\n",
    "\n",
    "### Edit One Letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letters(word, allow_switches = True):\n",
    "    edit_one_set = set()\n",
    "    \n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    if allow_switches:\n",
    "        edit_one_set.update(switch_letter(word))\n",
    "    \n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "    \n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input word at \nedit_one_l \n[\"'at\", \"'t\", 'a', \"a'\", \"a't\", 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', \"at'\", 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n\nNumber of outputs from edit_one_letter('at') is 134\n"
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letters(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Two Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    \n",
    "    edit_one = edit_one_letters(word, allow_switches=allow_switches)\n",
    "    \n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_two = edit_one_letters(w, allow_switches=allow_switches)\n",
    "            edit_two_set.update(edit_two)\n",
    "            \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of strings with edit distance of two: 2864\nFirst 10 strings ['', \"'\", \"''\", \"''a\", \"'a\", \"'a'\", \"'aa\", \"'ab\", \"'ac\", \"'ad\"]\nLast 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\nNumber of strings that are 2 edit distances from 'at' is 7726\n"
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Corrections\n",
    "Now we are ready to get corrections for the input string. We follow the following `Greedy Algorithm` to get corrections.\n",
    "\n",
    "1. If the word is in vocabulary, return the word (correct word)\n",
    "2. Else get all strings that are one edit away<br>\n",
    "    a. Check for strings in vocabulary<br>\n",
    "    b. If found, return strings<br>\n",
    "    c. Else Get all strings that are two edits away<br>\n",
    "        i. Check for strings in vocabulary<br>\n",
    "        ii. If found, return strings<br>\n",
    "        iii. Else return the original string (no correcctions available)\n",
    "\n",
    "The algorithm is an `Greedy Algorithm` as we are searching for strings that are at minimum edits from the input string at all time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n = 2, verbose = False):\n",
    "    suggestions = None\n",
    "    n_best = []\n",
    "    \n",
    "    if(word in vocab):\n",
    "        suggestions = [word]\n",
    "    else:\n",
    "        one_edit = edit_one_letters(word)\n",
    "        in_one_edit = filter(lambda x: x in vocab, one_edit)\n",
    "        if in_one_edit:\n",
    "            suggestions = list(in_one_edit)\n",
    "        else:\n",
    "            two_edit = edit_two_letters(word)\n",
    "            in_two_edit = filter(lambda x: x in vocab, two_edit)\n",
    "            if in_two_edit:\n",
    "                suggestions = list(in_two_edit)\n",
    "            else:\n",
    "                suggestions = [word]\n",
    "                \n",
    "    n_best = [[s, probs[s]] for s in suggestions]\n",
    "    n_best = sorted(n_best, key=lambda x: x[1], reverse=True)       \n",
    "    return n_best[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Corrections algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word : hai, probability : 0.012827\nWord : bhi, probability : 0.003414\nWord : hui, probability : 0.000785\n"
    }
   ],
   "source": [
    "my_word = 'hbi'\n",
    "n_best = 3\n",
    "vocab = list(vocab_word_count.keys())\n",
    "corrections = get_corrections(my_word, prob_dict, vocab, n = n_best)\n",
    "\n",
    "for i, word_probs in enumerate(corrections):\n",
    "    print(\"Word : {}, probability : {:.6f}\".format(word_probs[0], word_probs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "- deeplearning.ai - https://www.deeplearning.ai/natural-language-processing-specialization/"
   ]
  }
 ]
}